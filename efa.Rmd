<center><img src="https://github.com/PoliticayGobiernoPUCP/estadistica_anapol2/raw/master/PICS/LOGO_PUCP.png" width="500"></center>

<center> <header><h1>ESTADISTICA PARA EL ANALISIS POLITICO II</h1>  </header></center>

* Profesor:  <a href="http://www.pucp.edu.pe/profesor/jose-manuel-magallanes/" target="_blank">Dr. José Manuel Magallanes, Ph.D.</a> <br>
    - Profesor del Departamento de Ciencias Sociales, Sección de Ciencia Política y Gobierno.
    - [Oficina 105](https://goo.gl/maps/xuGeG6o9di1i1y5m6) - Edificio CISEPA / ECONOMIA / CCSS
    - Telefono: (51) 1 - 6262000 anexo 4302
    - Correo Electrónico: [jmagallanes@pucp.edu.pe](mailto:jmagallanes@pucp.edu.pe)
    

____

<center> <header><h2>EFA: Análisis Factorial Exploratorio</h2>  </header></center>
____



**Parte II**

Vamos a trabajar con las tablas de estos links.


* [https://en.wikipedia.org/wiki/World_Happiness_Report](https://en.wikipedia.org/wiki/World_Happiness_Report)

* [https://en.wikipedia.org/wiki/Democracy_Index](https://en.wikipedia.org/wiki/Democracy_Index)


```{r, echo=FALSE, eval=TRUE,warning=FALSE, message=FALSE}
library(htmltab)

# links
happyL=c("https://en.wikipedia.org/wiki/World_Happiness_Report",'//*[@id="mw-content-text"]/div/table/tbody')
demoL=c("https://en.wikipedia.org/wiki/Democracy_Index", '//*[@id="mw-content-text"]/div/table[2]/tbody')

# carga
happy = htmltab(doc = happyL[1],which  = happyL[2],encoding = "UTF-8")
demo  = htmltab(doc = demoL[1], which  = demoL[2], encoding = "UTF-8")

# limpieza
happy[,]=lapply(happy[,], trimws,whitespace = "[\\h\\v]") # no blanks
demo[,]=lapply(demo[,], trimws,whitespace = "[\\h\\v]") # no blanks

happy=na.omit(happy)
demo=na.omit(demo)

# formateo

library(stringr)
names(happy)=str_split(names(happy)," ",simplify = T)[,1]
names(happy)[names(happy)=="Score"]="ScoreHappy"

names(demo)=str_split(names(demo)," ",simplify = T)[,1]
names(demo)[names(demo)=="Score"]="ScoreDemo"
str(demo)

happy$Overall=NULL
demo[,c(1,9,10)]=NULL
HappyDemo=merge(happy,demo)
HappyDemo[,-1]=lapply(HappyDemo[,-1],as.numeric)
str(HappyDemo)
```
```{r}
theData=HappyDemo[,-c(1,2,9)]
fa.parallel(theData,fm = 'uls', fa = 'fa')
```

```{r}
library(GPArotation)
twofactor <- fa(theData,nfactors = 2,cor = 'mixed',rotate = "oblimin",fm="minres")
print(twofactor$loadings,cutoff=0.4)
```

```{r}
fa.diagram(twofactor)
```


```{r}

```




## Buscando estructura 

Muchas veces nos enfrentamos a _conjuntos_ de preguntas que desean consultar sobre un mismo tema. Ese mismo tema es una variable _latente_, la cuál es un concepto abstracto dificilmente visto de manera empírica, por lo que usaremos variables _observables_ para acercarnos a su comprensión.

Una primera técnica que lidia con esto es el análisis de componentes principales (**PCA** por sus siglas en inglés). El PCA simplemente va a reducir las preguntas y devolver menos dimensiones (en eso se parece a lo visto antes), la diferencia es que ahora lo usaremos con variables ordinales.

Traigamos una data de LAPOP:

```{r, eval=FALSE}
library(haven)
fileName='PeruLAPOP2014.sav'
fileToRead=file.path(folder,fileName)
dataSpss=read_sav(fileToRead)
```

Tenemos la data, como viene de encuesta, preguntemos cuantas columnas hay:
```{r, eval=FALSE}
ncol(dataSpss)
```


Con esta cantidad de columnas no vale la pena verlas todas aquí, pues lo que necesitamos es algunos conjuntos de variables que buscan un concepto. 

Veamos un primer conjunto de preguntas (B1-B47A):
![](lapopB.jpg)

Dónde estarán esas variables entre las 208 que tenemos?
```{r, eval=FALSE}
grep("b", colnames(dataSpss) )
```

Como deben ser consecutivos, veamos que hay de la columna 56 a la 73:
```{r, eval=FALSE}
colnames(dataSpss)[c(56:73)]
```


Aqui tenemos otro:

![](lapopN.jpg)

```{r, eval=FALSE}
# solo son 3:
labelsCol1=c('n9','n11','n15')
which(names(dataSpss)%in%labelsCol1)
```

Y un tercer grupo:

![](lapopPR.jpg)

Hacemos lo mismo:

```{r, eval=FALSE}
# solo son 3:
labelsCol2=c('pr3a','pr3b','pr3c','pr4')
which(names(dataSpss)%in%labelsCol2)
```

Ya sabemos cuáles son las columnas que queremos, pues procedemos a subsetear:

```{r, eval=FALSE}
set1=c(56:73)
set2=which(names(dataSpss)%in%labelsCol1)
set3=which(names(dataSpss)%in%labelsCol2)
#
columnasQueNecesito=c(set1,set2 ,set3)
sub_dataSpss=dataSpss[,columnasQueNecesito]

# ver:
head(sub_dataSpss)
```

Nos indican más arriba que no debemos aplicar la pregunta _b33_ a los de Lima. Será mejor eliminar esa columna:

```{r, eval=FALSE}
# cual es la columna?
which(names(sub_dataSpss)%in%'b33')
```

Esa es la columna que necesitamos eliminar:

```{r, eval=FALSE}
sub_dataSpss=sub_dataSpss[,-16]
```

Ahora sí, lo usual:

```{r, eval=FALSE}
# asegurarse que es data frame:
subTable=data.frame(sub_dataSpss)

# ver tipo de datos
str(subTable)
```

Ver valores:
```{r, eval=FALSE}
summary(subTable,maxsum = 8)
```

Ahora hay que reemplazar esos valores perdidos, como son data ordinal, debemos usar la mediana:
```{r, eval=FALSE}
for(i in 1:ncol(subTable)){  # para cada columna:
  MEDIANA=median(subTable[,i], na.rm = TRUE) # calcula la mediana de esa columna
  subTable[is.na(subTable[,i]), i] <- round(MEDIANA,0) # pon la mediana donde haya un NA en esa columna (redondeada)
}
```

Ya tenemos tres conjuntos de variables sin valores perdidos, aquí debemos hallar una matriz de correlación. Sin embargo, considerando, dado que los datos son ordinales, debemos usar la matriz policórica (en vez de la de correlación lineal de Pearson):

```{r, eval=FALSE}
library(psych)
Poly_cor <- polychoric(subTable)$rho
```

Hasta aquí todo marcha bien. Lo que queremos ver es si nuestros conjuntos de preguntas representaban conceptos bien diferenciados. Y esto sigue esta logica:

1) Saber si el tamaño de la muestra es el adecuado:

```{r, eval=FALSE}
KMO(Poly_cor)  #MSA: al menos 0.60, 0.9 es buenisimo.
```

2) Verificar que la matriz de correlaciones que hemos calculado NO sea la matriz de identidad:

```{r, eval=FALSE}
cortest.bartlett(Poly_cor, n=nrow(subTable))
```

Con esta información podemos saber qué variables están familiarizadas y diferenciadas:

```{r, eval=FALSE}
resultadoPr=principal(Poly_cor,3,rotate="varimax", scores=T)
print(resultadoPr,digits=3,cut = 0.4)
```

```{r, echo=F, eval=FALSE}
varCum=round(resultadoPr$Vaccounted[3,3],3)
```

Una información adicional se encuentra en la fila **Cummulative Var**, lo que nos interesa es el último valor de esa fila (0.452). Es decir, al elegir 3 latentes para representar a 24 variables, se ha podido recuperar el 45.2% de la variabilidad que las 24 representaban, o dicho de otra manera, las latentes pierden más del 50% de la varianza total. Claro, para que se llegue al 100%, necesitaríamos 24 variables latentes, lo cual nos dejaría como al principio.

```{r, echo=FALSE, eval=FALSE}
regresFactors=factor.scores(subTable,resultadoPr)$scores
```




https://www.promptcloud.com/blog/exploratory-factor-analysis-in-r/


_____
<br></br>

[al INICIO](#beginning)

[VOLVER A CONTENIDOS](https://politicaygobiernopucp.github.io/estadistica_anapol2/)
